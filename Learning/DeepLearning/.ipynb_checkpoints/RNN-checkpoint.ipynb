{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.1484375\n",
      "0.6875\n",
      "0.8046875\n",
      "0.7578125\n",
      "0.828125\n",
      "0.8046875\n",
      "0.8515625\n",
      "0.8515625\n",
      "0.90625\n",
      "0.90625\n",
      "0.8984375\n",
      "0.921875\n",
      "0.9296875\n",
      "0.921875\n",
      "0.9140625\n",
      "0.921875\n",
      "0.921875\n",
      "0.9453125\n",
      "0.8984375\n",
      "0.9296875\n",
      "0.9453125\n",
      "0.984375\n",
      "0.9375\n",
      "0.9296875\n",
      "0.9296875\n",
      "0.9453125\n",
      "0.953125\n",
      "0.9453125\n",
      "0.9609375\n",
      "0.9453125\n",
      "0.984375\n",
      "0.96875\n",
      "0.9609375\n",
      "0.9375\n",
      "0.9453125\n",
      "0.9609375\n",
      "0.9609375\n",
      "0.9765625\n",
      "0.9453125\n",
      "0.9453125\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "tf.reset_default_graph()\n",
    "# data \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# set hyperparameters\n",
    "learn_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "n_inputs = 28 # MNIST data input (image shape 28 * 28)\n",
    "n_steps = 28 # rnn cells numbers 一张图片每次输入28个pixes，需输入28次，按照时间顺序需要查看28步\n",
    "n_hidden_layers = 128 # 进入cells之前的隐藏层\n",
    "n_classes = 10 # cells输出之后的隐藏层，得到结果\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# define weights\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.truncated_normal([n_inputs, n_hidden_layers])), # 28 * 128\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden_layers, n_classes])) # 128 * 10\n",
    "}\n",
    "\n",
    "bias = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_layers])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes]))\n",
    "}\n",
    "\n",
    "def RNN(X, weights, bias):\n",
    "    # hidden layer for input to cell\n",
    "    ################################\n",
    "\n",
    "    # transpose the inputs shape from x --> (128batch * 28 steps, 28 inputs)\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "\n",
    "    # into hidden\n",
    "    X_in = tf.matmul(X, weights['in']) + bias['in'] # 矩阵乘法只能2维*2维，所以要先transpose，然后转回去\n",
    "\n",
    "    # retranspose\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_layers])\n",
    "\n",
    "    # cell\n",
    "    ################################\n",
    "    # forget_bias:LSTM的忘记系数，如果等于1，就是不会忘记任何信息。如果等于0，就都忘记\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_layers, forget_bias=0.0, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32) # 每一批的每一个是独立的\n",
    "    \n",
    "    '''\n",
    "    函数原型：\n",
    "    tf.nn.dynamic_rnn\n",
    "    (\n",
    "    cell,\n",
    "    inputs,\n",
    "    sequence_length=None,\n",
    "    initial_state=None,\n",
    "    dtype=None,\n",
    "    parallel_iterations=None,\n",
    "    swap_memory=False,\n",
    "    time_major=False,\n",
    "    scope=None\n",
    "    )\n",
    "    \n",
    "    Returns:\n",
    "    A pair (outputs, state) where:\n",
    "\n",
    "    outputs: The RNN output Tensor.\n",
    "\n",
    "    If time_major == False (default), this will be a Tensor shaped: [batch_size, max_time, cell.output_size].\n",
    "    If time_major == True, this will be a Tensor shaped: [max_time, batch_size, cell.output_size].\n",
    "\n",
    "    Note：\n",
    "    if cell.output_size is a (possibly nested) tuple of integers or TensorShape objects, \n",
    "    then outputs will be a tuple having the same structure as cell.output_size, \n",
    "    containing Tensors having shapes corresponding to the shape data in cell.output_size.\n",
    "\n",
    "    state: The final state. \n",
    "    If cell.state_size is an int, this will be shaped [batch_size, cell.state_size]. \n",
    "    If it is a TensorShape, this will be shaped [batch_size] + cell.state_size. \n",
    "    If it is a (possibly nested) tuple of ints or TensorShape, this will be a tuple having the corresponding shapes. \n",
    "    If cells are LSTMCells state will be a tuple containing a LSTMStateTuple for each cell.\n",
    "    lstm 来说, state可被分为(c_state, h_state) 【主线state， 支线state】\n",
    "\n",
    "    '''\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "    # hidden layer for output as the final results\n",
    "    # method1: \n",
    "    # result = tf.matmul(final_state[1], weights['out']) + bias['out'] # final_state:(major state, side state)\n",
    "    # method2:\n",
    "    '''\n",
    "    tf.transpose([1, 0, 2]) :交换坐标轴， 第0维和第1维交换\n",
    "    tf.unstack(): 矩阵分解函数，默认axis=0，分解成一个list,降纬\n",
    "    '''\n",
    "    \n",
    "    # outputs: shape=[batch_size, n_step, n_hidden_layers]\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1, 0, 2]))\n",
    "    # output[-1]=final_state, shape=[batch_size, n_hidden_layers]\n",
    "    results = tf.matmul(outputs[-1], weights['out']) + bias['out']\n",
    "\n",
    "    return results\n",
    "\n",
    "pred = RNN(x, weights, bias)\n",
    "# logits, logits=log(p/1-p)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))  \n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)), dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    times = 0 \n",
    "    while times*batch_size < training_iters:\n",
    " \n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        test_x, test_y = mnist.test.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape([batch_size, n_steps, n_inputs])\n",
    "        test_x = test_x.reshape([batch_size, n_steps, n_inputs])\n",
    "        sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
    "        if times % 20 == 0:\n",
    "            print(sess.run(accuracy, feed_dict={x: test_x, y: test_y}))\n",
    "        times+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# %matplotlib inline\n",
    "# tf.reset_default_graph()\n",
    "# seq2seq model\n",
    "\n",
    "# hyper-parameters\n",
    "\n",
    "BATCH_START = 0   # 第几批batch\n",
    "TIME_STEPS = 20 # 共有20个重复的rnn cells\n",
    "BATCH_SIZE = 50 # 每一批有50个samples\n",
    "INPUT_SIZE = 1 # 每一次传入一个数据\n",
    "OUTPUT_SIZE = 1 # 每一次输出一个数据\n",
    "CELL_SIZE = 20 # rnn cells的隐藏层有20个neurons\n",
    "LR = 0.005 # 学习率\n",
    "\n",
    "def get_bacth():\n",
    "    global BATCH_START, TIME_STEPS\n",
    "    xs = np.arange(BATCH_START, TIME_STEPS*BATCH_SIZE + BATCH_START).reshape((BATCH_SIZE, TIME_STEPS))\n",
    "#     xs = xs / (10 * np.pi)\n",
    "    seq = np.sin(xs)\n",
    "    res = np.cos(xs)\n",
    "    BATCH_START += TIME_STEPS\n",
    "    # seq.shape(BATCH_SIZE, TIME_STEPS) --> seq.shape(BATCH_SIZE, TIME_STEPS, 1)\n",
    "    seq = seq[:, :, np.newaxis]\n",
    "    res = res[:, :, np.newaxis]\n",
    "    return [seq, res, xs]\n",
    "\n",
    "\n",
    "class LSTMRNN(object):\n",
    "    def __init__(self, n_steps, input_size, output_size, cell_size, batch_size):\n",
    "        self.n_steps = n_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.cell_size = cell_size\n",
    "        self.batch_size = batch_size\n",
    "        with tf.name_scope('Inputs'):\n",
    "            self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name='xs')\n",
    "            self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name='ys')\n",
    "\n",
    "        with tf.variable_scope('In_hidden'):\n",
    "            self.add_input_layer()\n",
    "        with tf.variable_scope('LSTM_cell'):\n",
    "            self.add_cells()\n",
    "        with tf.variable_scope('Out_hidden'):\n",
    "            self.add_output_layer()\n",
    "\n",
    "        with tf.name_scope('Cost'):\n",
    "            self.compute_cost()\n",
    "        with tf.name_scope('Train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)\n",
    "            \n",
    "        '''\n",
    "        tf.name_scope()中：\n",
    "        1.tf.Variable(): \t定义的时候, 假如name都一样, 但输出的变量名并不是一样的，本质上他们并不是一样的变量. \n",
    "        2.tf.get_variable()：定义的变量不会被tf.name_scope()当中的名字所影响.\n",
    "\n",
    "        tf.variable_scope()中：\n",
    "        1.可以重复利用变量\n",
    "        2.tf.Variable()每次都会产生新的变量\n",
    "        3.tf.get_variable()如果遇到了同样名字的变量时, 它会单纯的提取这个同样名字的变量(避免产生新变量). \n",
    "        4.而在重复使用的时候, 一定要在代码中强调 scope.reuse_variables(), 否则系统将会报错.\n",
    "        '''\n",
    "\n",
    "    def add_input_layer(self):\n",
    "        l_in_x = tf.reshape(self.xs, [-1, self.input_size], name='2_2D')\n",
    "        ws_in = self._weight_variable([self.input_size, self.cell_size])\n",
    "        bs_in = self._bias_variable([self.cell_size])\n",
    "        with tf.name_scope('WX_PLUS_B'):\n",
    "            l_in_y = tf.matmul(l_in_x, ws_in) + bs_in\n",
    "        self.l_in_y = tf.reshape(l_in_y, [-1, self.n_steps, self.cell_size], name='2_3D')\n",
    "\n",
    "    def add_cells(self):\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size)\n",
    "        with tf.name_scope('Initial_state'):\n",
    "            self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        self.cell_outputs, self.final_state = tf.nn.dynamic_rnn(lstm_cell, self.l_in_y, \n",
    "                                                initial_state=self.cell_init_state, time_major=False)\n",
    "\n",
    "    def add_output_layer(self):\n",
    "        l_out_x = tf.reshape(self.cell_outputs, [-1, self.cell_size], name='2_2D')\n",
    "        wx_out = self._weight_variable([self.cell_size, self.output_size])\n",
    "        bs_out = self._bias_variable([self.output_size])\n",
    "        with tf.name_scope('WX_PLUS_B'):\n",
    "            self.pred = tf.matmul(l_out_x, wx_out) + bs_out\n",
    "\n",
    "    def compute_cost(self):\n",
    "        '''\n",
    "        logits \n",
    "        targets \n",
    "        weights\n",
    "        '''\n",
    "        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [tf.reshape(self.pred, [-1], name='reshaped_pred')],\n",
    "            [tf.reshape(self.ys, [-1], name='reshape_target')],\n",
    "            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],\n",
    "            average_across_timesteps=True,\n",
    "            softmax_loss_function=self.ms_error,\n",
    "            name='Losses'\n",
    "            )\n",
    "        with tf.name_scope('Average_Cost'):\n",
    "            self.cost = tf.div(tf.reduce_sum(losses, name='Losses_sum'), self.batch_size, name='Average_Cost')\n",
    "            tf.summary.scalar('Cost', self.cost)\n",
    "\n",
    "    # staticmethod将class中的方法变成静态方法，可以当做普通方法一样调用,而不会将类实例本身作为第一个self参数传给方法\n",
    "    @staticmethod\n",
    "    def ms_error(labels, logits):\n",
    "        return tf.square(tf.subtract(labels, logits))\n",
    "\n",
    "    def _weight_variable(self, shape, name='Weights'):\n",
    "        initializer = tf.truncated_normal(shape,0, 0.5)\n",
    "        return tf.get_variable(initializer=initializer, name=name)\n",
    "\n",
    "    def _bias_variable(self, shape, name='Bias'):\n",
    "        initializer = tf.truncated_normal(shape, 0, 0.5)\n",
    "        return tf.get_variable(name=name, initializer=initializer)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)\n",
    "sess = tf.Session()\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "plt.ion()\n",
    "plt.show()\n",
    "for i in range(100):\n",
    "    seq, res, xs = get_bacth()\n",
    "    if i == 0:\n",
    "        feed_dict = {model.xs: seq, model.ys: res}\n",
    "    else:\n",
    "        feed_dict = {model.xs: seq, model.ys: res, model.cell_init_state: state}\n",
    "    _, cost, state, pred = sess.run(\n",
    "        [model.train_op, model.cost, model.final_state, model.pred],\n",
    "        feed_dict=feed_dict\n",
    "        )\n",
    "\n",
    "    # plotting\n",
    "    plt.plot(xs[0, :], res[0].flatten(), 'r',\n",
    "             xs[0, :], pred.flatten()[:TIME_STEPS], 'b--')\n",
    "    plt.ylim((-1.2, 1.2))\n",
    "    plt.draw()\n",
    "    plt.pause(0.3)\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print('Cost: ', round(cost, 4))\n",
    "        result = sess.run(merged, feed_dict=feed_dict)\n",
    "        writer.add_summary(result, i)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Hyper-parameters\n",
    "BATCH_START = 0\n",
    "BATCH_SIZE = 20\n",
    "TIME_STEP = 15\n",
    "INPUT_SIZE = 1\n",
    "CELL_SIZE = 10\n",
    "OUTPUT_SIZE = 1\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "def get_batch():\n",
    "    global BATCH_START\n",
    "    x = np.arange(BATCH_START, BATCH_SIZE*TIME_STEP*INPUT_SIZE + BATCH_START).reshape((BATCH_SIZE, TIME_STEP, INPUT_SIZE))\n",
    "    x = x / (5 * np.pi)\n",
    "    y = np.sin(x)\n",
    "    seq = np.cos(x)\n",
    "    BATCH_START += TIME_STEP*INPUT_SIZE\n",
    "    return seq, y, x\n",
    "\n",
    "\n",
    "\n",
    "class LSTMRNN(object):\n",
    "    def __init__(self, batch_size, time_step, input_size, cell_size, output_size, lr):\n",
    "        self.batch_size = batch_size\n",
    "        self.time_step = time_step\n",
    "        self.input_size = input_size\n",
    "        self.cell_size = cell_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        with tf.name_scope('Inputs'):\n",
    "            self.xs = tf.placeholder(tf.float32, [None, self.time_step, self.input_size], name='xs')\n",
    "            self.ys = tf.placeholder(tf.float32, [None, self.time_step, self.output_size], name='ys')\n",
    "            \n",
    "        with tf.variable_scope('InputLayer'):\n",
    "            self.add_input_layer()\n",
    "        with tf.variable_scope('Cells'):\n",
    "            self.add_cells()\n",
    "        with tf.variable_scope('OutputLayer'):\n",
    "            self.add_output_layer()\n",
    "                    \n",
    "        with tf.name_scope('Cost'):\n",
    "            self.calc_loss()\n",
    "        with tf.name_scope('Train_Op'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.cost)\n",
    "    \n",
    "    def add_input_layer(self):\n",
    "        l_in = tf.reshape(self.xs, [-1, self.input_size])\n",
    "        ws = self._weight_variable([self.input_size, self.cell_size])\n",
    "        bs = self._bias_variable([self.cell_size])\n",
    "        rnn_inputs = tf.matmul(l_in, ws) + bs\n",
    "        self.rnn_inputs = tf.reshape(rnn_inputs, [-1, self.time_step, self.cell_size])\n",
    "        \n",
    "    \n",
    "    def add_cells(self):\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias=0.7)\n",
    "        self.init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        self.output, self.final_state = tf.nn.dynamic_rnn(\n",
    "            lstm_cell,\n",
    "            self.rnn_inputs,\n",
    "            initial_state=self.init_state,\n",
    "            time_major=False # if time_major==False, input must to be [batch_size, time_step, ...]\n",
    "        )\n",
    "    \n",
    "    def add_output_layer(self):\n",
    "        l_output = tf.reshape(self.output, [-1, self.cell_size])\n",
    "        wo = self._weight_variable([self.cell_size, self.output_size])\n",
    "        bo = self._bias_variable([self.output_size])\n",
    "        out = tf.matmul(l_output, wo) + bo\n",
    "        self.pred = tf.reshape(out, [-1, self.time_step, self.output_size])\n",
    "    \n",
    "    def calc_loss(self):\n",
    "        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [tf.reshape(self.pred, [-1])],\n",
    "            [tf.reshape(self.ys, [-1])],\n",
    "            [tf.ones([self.batch_size * self.time_step], dtype=tf.float32)],\n",
    "            softmax_loss_function=self.ms_error\n",
    "        )\n",
    "        with tf.name_scope('AverBatchCost'):\n",
    "            self.cost = tf.div(tf.reduce_sum(losses), self.batch_size, name='Cost')\n",
    "            tf.summary.scalar('Cost', self.cost)\n",
    "        \n",
    "    @staticmethod\n",
    "    def ms_error(logits, labels):\n",
    "        return tf.square(tf.subtract(logits, labels))\n",
    "    \n",
    "    def _weight_variable(self, shape, name='Weights'):\n",
    "        init = tf.truncated_normal(shape, 0, 0.5)\n",
    "        return tf.get_variable(initializer=init, name=name)\n",
    "    \n",
    "    def _bias_variable(self, shape, name='Bias'):\n",
    "        init = tf.truncated_normal(shape, 0, 0.5)\n",
    "        return tf.get_variable(initializer=init, name=name)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    model = LSTMRNN(BATCH_SIZE, TIME_STEP, INPUT_SIZE, CELL_SIZE, OUTPUT_SIZE, LEARNING_RATE)\n",
    "    sess = tf.Session()\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('Logs/', sess.graph)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    plt.ion()\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(500):\n",
    "        seq, label, x = get_batch()\n",
    "        if i == 0:\n",
    "            feed_dict = {model.xs: seq, model.ys: label}\n",
    "        else:\n",
    "            feed_dict = {model.xs: seq, model.ys: label, model.init_state: state}\n",
    "        _, cost, state, pred = sess.run(\n",
    "            [model.train_op, model.cost, model.final_state, model.pred],\n",
    "            feed_dict=feed_dict\n",
    "        )\n",
    "        # plotting\n",
    "        plt.plot(x[0, :, :], label.flatten()[: TIME_STEP], 'r',\n",
    "                 x[0, :, :], pred.flatten()[: TIME_STEP], 'b--')\n",
    "        plt.ylim((-1.2, 1.2))\n",
    "        plt.draw()\n",
    "        plt.pause(0.3)\n",
    "        if i % 20 == 0:\n",
    "            print('Cost: ', round(cost, 4))\n",
    "            result = sess.run(merged, feed_dict=feed_dict)\n",
    "            writer.add_summary(result, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
