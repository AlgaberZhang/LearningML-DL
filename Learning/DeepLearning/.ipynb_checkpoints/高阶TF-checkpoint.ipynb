{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "0.1404\n",
      "0.639\n",
      "0.7256\n",
      "0.7758\n",
      "0.8013\n",
      "0.8107\n",
      "0.8264\n",
      "0.8376\n",
      "0.8437\n",
      "0.8497\n",
      "0.8532\n",
      "0.8549\n",
      "0.8599\n",
      "0.858\n",
      "0.8641\n",
      "0.8648\n",
      "0.8663\n",
      "0.8688\n",
      "0.8719\n",
      "0.8729\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# number 1 to 10 data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 构建层\n",
    "    def add_layer(inputs, input_size, output_size, activation_function):\n",
    "        with tf.name_scope('Layer'):\n",
    "            with tf.name_scope('Weights'):\n",
    "                Weights = tf.Variable(tf.random_normal([input_size, output_size]))\n",
    "                tf.summary.histogram('Weights', Weights)\n",
    "            with tf.name_scope('Bias'):\n",
    "                bias = tf.Variable(tf.zeros([1, output_size]) + 0.1)\n",
    "                tf.summary.histogram('Bias', bias)\n",
    "            wx_b = tf.add(tf.matmul(inputs, Weights), bias)\n",
    "            if activation_function != None:\n",
    "                output = activation_function(wx_b)\n",
    "            else:\n",
    "                output = wx_b\n",
    "            tf.summary.histogram('Output', output)\n",
    "        return output\n",
    "\n",
    "    # 定义准确率计算\n",
    "    def computer_accuracy(v_x, v_y):\n",
    "        global prediction\n",
    "        y_pre = sess.run(prediction, feed_dict={xs: v_x})\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(v_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "        result = sess.run(accuracy, feed_dict={xs: v_x, ys: v_y})\n",
    "        return result\n",
    "\n",
    "\n",
    "    # 搭建网络 \n",
    "    xs = tf.placeholder(tf.float32, [None, 784]) # 28*28\n",
    "    ys = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    prediction = add_layer(xs, 784, 10, tf.nn.softmax)\n",
    "\n",
    "    # 分类问题用交叉熵\n",
    "    with tf.name_scope('CrossEntrphy'):\n",
    "        cross_entrypy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1]))\n",
    "        tf.summary.scalar('CrossEntrpyh', cross_entrypy)\n",
    "\n",
    "    train = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entrypy)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:        \n",
    "        sess.run(init)\n",
    "        writer = tf.summary.FileWriter('./logs', sess.graph)  \n",
    "        mergerd = tf.summary.merge_all()         \n",
    "        for i in range(1000):\n",
    "            batch_x, batch_y = mnist.train.next_batch(100)\n",
    "            sess.run(train, feed_dict={xs: batch_x, ys: batch_y})\n",
    "            if i % 50 == 0:\n",
    "                print(computer_accuracy(mnist.test.images, mnist.test.labels))\n",
    "                rs = sess.run(mergerd, feed_dict={xs: batch_x, ys: batch_y})\n",
    "                writer.add_summary(rs, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-0d5fcc9a3568>:20: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# load data\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "y = LabelBinarizer().fit_transform(y) # 把数字变成独热编码，二值化变成（0，1）\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "def add_layer(inputs, input_size, output_size, layer_name, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random.normal([input_size, output_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, output_size]) + 0.1)\n",
    "    Wx_plus_bias = tf.add(tf.matmul(inputs, Weights), biases)\n",
    "    \n",
    "    # add dropout\n",
    "    Wx_plus_bias = tf.nn.dropout(Wx_plus_bias, keep_prob)\n",
    "    if activation_function == None:\n",
    "        outputs = Wx_plus_bias\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_bias)\n",
    "    return outputs\n",
    "\n",
    "# define placeholder\n",
    "xs = tf.placeholder(tf.float32, [None, 64])\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# add hidden layer\n",
    "l1 = add_layer(xs, 64, 50, 'layer1', tf.nn.tanh) # 神经元个数不能过多，activation_function用tf.nn.tanh，否则数据会变成Nan\n",
    "prediction = add_layer(l1, 50, 10, 'layer2', tf.nn.softmax)\n",
    "\n",
    "# loss\n",
    "cross_entropy = tf.reduce_mean( -tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1]))\n",
    "tf.summary.scalar('CrossEntropy', cross_entropy)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "mergerd = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter('logs/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('logs/test', sess.graph)\n",
    "    for i in range(1000):\n",
    "        sess.run(train, feed_dict={xs: X_train, ys: y_train, keep_prob: 0.5})\n",
    "        if i % 50 == 0:\n",
    "            train_result = sess.run(mergerd, feed_dict={xs: X_train, ys: y_train, keep_prob: 1.0})\n",
    "            test_result = sess.run(mergerd, feed_dict={xs: X_test, ys: y_test, keep_prob: 1.0})\n",
    "            train_writer.add_summary(train_result, i)\n",
    "            test_writer.add_summary(test_result, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-43062867bc7c>:13: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "tf.truncated_normal(shape, mean, stddev)\n",
    "1.截断的产生正太分布的函数。\n",
    "2.就是说产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。\n",
    "3.和一般的正太分布的产生随机数据比起来，这个函数产生的随机数与均值的差距不会超过两倍的标准差，但是一般的别的函数是可能的。\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#number 1 to 10 data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "def accuracy(v_x, v_y):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_x, ys: v_y, keep_prob: 1.0})\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(v_y, 1), tf.argmax(y_pre, 1)), dtype=tf.float32))\n",
    "    ret = sess.run(acc, feed_dict={xs: v_x, ys: v_y, keep_prob: 1.0})                        \n",
    "    return ret\n",
    "\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, 0, 0.1))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "def con2d(x, ft): \n",
    "    '''\n",
    "    input: shape为[batch, in_height, in_weight, in_channel]\n",
    "    \n",
    "    filter：卷积核，要求也是一个张量，shape为 [ filter_height, filter_weight, in_channel, out_channels ]，\n",
    "            其中 filter_height 为卷积核高度，filter_weight 为卷积核宽度;\n",
    "            in_channel 是图像通道数,和 input 的 in_channel 要保持一致\n",
    "            out_channel 是卷积核数量。\n",
    "    \n",
    "    strides：卷积时在图像每一维的步长，这是一个一维的向量，[ 1, strides, strides, 1]，第一位和最后一位固定必须是1\n",
    "    \n",
    "    padding：string类型，值为“SAME”和 “VALID”，表示的是卷积的形式，是否考虑边界；\n",
    "             \"SAME\"是考虑边界，不足的时候用0去填充周围，\"VALID\"则不考虑。\n",
    "    '''\n",
    "    return tf.nn.conv2d(x, ft, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    '''\n",
    "    tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
    "    \n",
    "    value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，\n",
    "            依然是[batch, height, width, channels]这样的shape\n",
    "\n",
    "    ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1\n",
    "\n",
    "    strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
    "\n",
    "    padding：和卷积类似，可以取'VALID' 或者'SAME'\n",
    "\n",
    "    返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式\n",
    "\n",
    "    '''\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None, 28*28])\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x_input = tf.reshape(xs, [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "# con1\n",
    "filter1 = weight_variable([5, 5, 1, 32]) # patch 5 * 5, in size 1; out size 32\n",
    "bias1 = bias_variable([32])\n",
    "h1 = tf.nn.relu(con2d(x_input, filter1) + bias1)\n",
    "h1_pool = max_pool_2x2(h1) #  14*14*32\n",
    "\n",
    "# con2\n",
    "filter2 = weight_variable([5, 5, 32, 64])\n",
    "bias2 = bias_variable([64])\n",
    "h2 = tf.nn.relu(con2d(h1_pool, filter2) + bias2)\n",
    "h2_pool = max_pool_2x2(h2) # 7*7*64\n",
    "\n",
    "# fc1\n",
    "w_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "# h2_pool:shape[samples, 7, 7, 64] --> [samples, 7*7*64] 三维变成一维，flat\n",
    "h2_pool_flat = tf.reshape(h2_pool, [-1, 7*7*64])\n",
    "fc1 = tf.nn.relu(tf.matmul(h2_pool_flat, w_fc1) + b_fc1)\n",
    "fc1_drop = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "# fc2\n",
    "w_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "prediction = tf.nn.softmax(tf.matmul(fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "loss = tf.reduce_mean( -tf.reduce_sum(ys*tf.log(prediction), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     for i in range(1000):\n",
    "#         batch_x, batch_y = mnist.train.next_batch(100)\n",
    "#         sess.run(train_step, feed_dict={xs: batch_x, ys: batch_y, keep_prob: 0.7})\n",
    "#         if i % 100 == 0:\n",
    "#             print(accuracy(mnist.test.images[:1000], mnist.test.labels[:1000]))\n",
    "#     saver.save(sess, 'models/mnist.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/mnist.ckpt\n",
      "0.979\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('models/mnist.ckpt.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('models/'))\n",
    "    print(accuracy(mnist.test.images[4000:5000], mnist.test.labels[4000:5000]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
