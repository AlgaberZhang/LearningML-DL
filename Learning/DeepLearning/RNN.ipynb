{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.0859375\n",
      "0.5703125\n",
      "0.78125\n",
      "0.796875\n",
      "0.8359375\n",
      "0.8203125\n",
      "0.84375\n",
      "0.8515625\n",
      "0.84375\n",
      "0.9375\n",
      "0.9375\n",
      "0.890625\n",
      "0.9453125\n",
      "0.890625\n",
      "0.9609375\n",
      "0.9296875\n",
      "0.9375\n",
      "0.953125\n",
      "0.8984375\n",
      "0.9296875\n",
      "0.9375\n",
      "0.9609375\n",
      "0.90625\n",
      "0.9921875\n",
      "0.953125\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.96875\n",
      "0.9765625\n",
      "0.9453125\n",
      "0.96875\n",
      "0.953125\n",
      "0.9375\n",
      "0.9453125\n",
      "0.984375\n",
      "0.984375\n",
      "0.9921875\n",
      "0.9375\n",
      "0.9609375\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "tf.reset_default_graph()\n",
    "# data \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# set hyperparameters\n",
    "learn_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "n_inputs = 28 # MNIST data input (image shape 28 * 28)\n",
    "n_steps = 28 # rnn cells numbers 一张图片每次输入28个pixes，需输入28次，按照时间顺序需要查看28步\n",
    "n_hidden_layers = 128 # 进入cells之前的隐藏层\n",
    "n_classes = 10 # cells输出之后的隐藏层，得到结果\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# define weights\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.truncated_normal([n_inputs, n_hidden_layers])), # 28 * 128\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden_layers, n_classes])) # 128 * 10\n",
    "}\n",
    "\n",
    "bias = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_layers])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes]))\n",
    "}\n",
    "\n",
    "def RNN(X, weights, bias):\n",
    "    # hidden layer for input to cell\n",
    "    ################################\n",
    "\n",
    "    # transpose the inputs shape from x --> (128batch * 28 steps, 28 inputs)\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "\n",
    "    # into hidden\n",
    "    X_in = tf.matmul(X, weights['in']) + bias['in'] # 矩阵乘法只能2维*2维，所以要先transpose，然后转回去\n",
    "\n",
    "    # retranspose\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_layers])\n",
    "\n",
    "    # cell\n",
    "    ################################\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_layers, forget_bias=1.0, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32) # 每一批的每一个是独立的\n",
    "    \n",
    "    '''\n",
    "    函数原型：\n",
    "    tf.nn.dynamic_rnn\n",
    "    (\n",
    "    cell,\n",
    "    inputs,\n",
    "    sequence_length=None,\n",
    "    initial_state=None,\n",
    "    dtype=None,\n",
    "    parallel_iterations=None,\n",
    "    swap_memory=False,\n",
    "    time_major=False,\n",
    "    scope=None\n",
    "    )\n",
    "    \n",
    "    Returns:\n",
    "    A pair (outputs, state) where:\n",
    "\n",
    "    outputs: The RNN output Tensor.\n",
    "\n",
    "    If time_major == False (default), this will be a Tensor shaped: [batch_size, max_time, cell.output_size].\n",
    "    If time_major == True, this will be a Tensor shaped: [max_time, batch_size, cell.output_size].\n",
    "\n",
    "    Note：\n",
    "    if cell.output_size is a (possibly nested) tuple of integers or TensorShape objects, \n",
    "    then outputs will be a tuple having the same structure as cell.output_size, \n",
    "    containing Tensors having shapes corresponding to the shape data in cell.output_size.\n",
    "\n",
    "    state: The final state. \n",
    "    If cell.state_size is an int, this will be shaped [batch_size, cell.state_size]. \n",
    "    If it is a TensorShape, this will be shaped [batch_size] + cell.state_size. \n",
    "    If it is a (possibly nested) tuple of ints or TensorShape, this will be a tuple having the corresponding shapes. \n",
    "    If cells are LSTMCells state will be a tuple containing a LSTMStateTuple for each cell.\n",
    "    lstm 来说, state可被分为(c_state, h_state) 【主线state， 支线state】\n",
    "\n",
    "    '''\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "    # hidden layer for output as the final results\n",
    "    # method1: \n",
    "    # result = tf.matmul(final_state[1], weights['out']) + bias['out'] # final_state:(major state, side state)\n",
    "    # method2:\n",
    "    '''\n",
    "    tf.transpose([1, 0, 2]) :交换坐标轴， 第0维和第1维交换\n",
    "    tf.unstack(): 矩阵分解函数，默认axis=0，分解成一个list,降纬\n",
    "    '''\n",
    "    \n",
    "    # outputs: shape=[batch_size, n_step, n_hidden_layers]\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1, 0, 2]))\n",
    "    # output[-1]=final_state, shape=[batch_size, n_hidden_layers]\n",
    "    results = tf.matmul(outputs[-1], weights['out']) + bias['out']\n",
    "\n",
    "    return results\n",
    "\n",
    "pred = RNN(x, weights, bias)\n",
    "# logits, logits=log(p/1-p)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))  \n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)), dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    times = 0 \n",
    "    while times*batch_size < training_iters:\n",
    " \n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        test_x, test_y = mnist.test.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape([batch_size, n_steps, n_inputs])\n",
    "        test_x = test_x.reshape([batch_size, n_steps, n_inputs])\n",
    "        sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
    "        if times % 20 == 0:\n",
    "            print(sess.run(accuracy, feed_dict={x: test_x, y: test_y}))\n",
    "        times+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
